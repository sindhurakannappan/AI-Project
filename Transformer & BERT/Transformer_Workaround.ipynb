{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TRANSFORMER - ATTENTION IS ALL YOU NEED!!!**\n",
        "\n",
        " Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between Jax, PyTorch and TensorFlow.\n",
        "\n",
        " **Refer:**\n",
        "\n",
        " https://huggingface.co/docs/transformers/index\n",
        "\n",
        "  [ jay alammar ](https://jalammar.github.io/illustrated-bert/)\n",
        "\n",
        " [ deepset AI ](https://www.deepset.ai/blog/the-definitive-guide-to-bertmodels)\n",
        "\n",
        " https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c\n"
      ],
      "metadata": {
        "id": "x3_6oJeLbHox"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsFWSTQeJnrn",
        "collapsed": true
      },
      "source": [
        "# Transformers installation\n",
        "# ! pip install transformers\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hcFZrgLJnru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99e0010-e244-4e95-87a7-9665f78d47f1",
        "collapsed": true
      },
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline('sentiment-analysis')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-GywL7wJnru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30033260-c97e-4400-b5cb-08ac36e7977a"
      },
      "source": [
        "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdElUZMW3G7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d82ca3-0205-4f97-c44f-48fdfc8146b1"
      },
      "source": [
        "classifier('The pizza is not that great but the crust is awesome.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998461008071899}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJFceBDqJnrv"
      },
      "source": [
        "That's encouraging! You can use it on a list of sentences, which will be preprocessed then fed to the model as a\n",
        "*batch*, returning a list of dictionaries like this one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQRoxvRcJnrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c2f636-69d1-4dc9-d804-c3c03a64d1d7"
      },
      "source": [
        "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
        "           \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: POSITIVE, with score: 0.9998\n",
            "label: NEGATIVE, with score: 0.5309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline with Model specific"
      ],
      "metadata": {
        "id": "zKBWjUVADr_L"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEJhsHatJnrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31382aa3-a44d-4ad4-bfa8-ab5f555539f6",
        "collapsed": true
      },
      "source": [
        "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3krTII5i4vhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e82026-2700-41f5-d34f-a0e7cb42cdf8"
      },
      "source": [
        "classifier(\"Esperamos que no lo odie.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': '3 stars', 'score': 0.33688199520111084}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U99shHSYJnrw"
      },
      "source": [
        "This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also\n",
        "replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model\n",
        "object and its associated tokenizer.\n",
        "\n",
        "We will need two classes for this. The first is `AutoTokenizer`, which we will use to download the\n",
        "tokenizer associated to the model we picked and instantiate it. The second is\n",
        "`AutoModelForSequenceClassification` (or\n",
        "`TFAutoModelForSequenceClassification` if you are using TensorFlow), which we will use to download\n",
        "the model itself. Note that if we were using the library on an other task, the class of the model would change. The\n",
        "[task summary](https://huggingface.co/transformers/task_summary.html) tutorial summarizes which class is used for which task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5JM68EAJnrw"
      },
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBsiWPIzJnrx"
      },
      "source": [
        "Now, to download the models and tokenizer we found previously, we just have to use the\n",
        "`AutoModelForSequenceClassification.from_pretrained` method (feel free to replace `model_name` by\n",
        "any other model from the model hub):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUYjEsjLJnrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a027566-f400-403d-8bdb-f2ec5e6f3f25"
      },
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGqyy19F6HhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72945ad-8b2c-49c5-f426-5afb58e888fc"
      },
      "source": [
        "classifier(\"I am a good boy\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': '4 stars', 'score': 0.4229269027709961}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYijwmcZJnrx"
      },
      "source": [
        "If you don't find a model that has been pretrained on some data similar to yours, you will need to fine-tune a\n",
        "pretrained model on your data. We provide [example scripts](https://huggingface.co/transformers/examples.html) to do so. Once you're done, don't forget\n",
        "to share your fine-tuned model on the hub with the community, using [this tutorial](https://huggingface.co/transformers/model_sharing.html)."
      ]
    }
  ]
}